#!/bin/bash

banner() {

 
    while read -r line; do
    	printf "\033[1;31m\e[%s\033[0m\n" "$line"
    done <<-EOF
    _    ____                      ;
    _   /---.'.__             ____//
    _        '--.\           /.---'
    _   _______  \!         //
    _ /.------.\  \|      .'/  ______
    _//  ___  \ \ ||/|\  //  _/_----.\__
    |/  /.-.\  \ \:|< >|// _/.'..\   '--'
    '  //   !'. | !'.|.'/ /_/ /  \!
    _ //     \ \_\!" ' ~\-'.-'    \!
    _//       '-._| :H: |'-.__     \!
    //           (!'==='!)'-._\     ||
    ||                        \!    \|
    ||      crawl@synacktra    \!    '
    |/                          \!
    '     Author: Harsh Verma    ||
    _                            ||
    _                            \|
    _                             '
EOF
printf "see help menu: %s -h\n" "${0##*/}"
}

[ -z "$*" ] && banner

#display error message and exit
die () {
    printf "\033[31m%s\033[0m\n" "$*" >&2
    exit 1
}

help_text () {

    while read -r line; do
        printf "%s\n" "$line"
    done <<-EOF
	
    |Usage:
    |  ${0##*/} [-f] (href|script) 
    |
    |Options:
    |  -h show help menu
    |  -f fetch a type of link. [href|script]
    |
    |Example:
    |  crawl -f script [domain].[TLD]
    |  crawl [domain].[TLD]/directory
    |  crawl -f href [domain].[TLD]/directory?key=value
    |
    |By default, crawl fetches both href and script links
EOF
}

fetch="(href|<script.*src)"
group=3

while getopts 'hf:' OPT; do
    case $OPT in
        f) 
            group=2
            case "$OPTARG" in
		script)
		    fetch="<script.*src" ;;
		href)
		    fetch="href" ;;
		*)
		    help_text
		    exit 1
		    ;;
	     esac
	     ;;
	 h|*)
	 help_text
	 exit 0
	 ;;
    esac
done

[ "$#" -eq 2 ] && die "No URL detected!"

domain=$(printf "%s" "${@: -1}" | sed -E "s!^http[s]?://!!g")
url="https://$domain"
user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"

crawl() {

    base_url="https://$(printf "%s" "$1" | cut -d '/' -f 3)"
	
    curl -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' \
         -H 'Accept-Language: en-GB,en-US;q=0.8,en;q=0.6' -A "$user_agent" -sL "$1" | \
         grep -Eo "$fetch=[\"'][^\"']+"  | sed -E -e "s@$fetch=[\"'](#.*|/)?(.*)@\\$group@g" \
	 -e "s^[[:space:]]^%20^g" -e "/^(http|tel)/!s!.*!$base_url\/&!"

}


cache=($(crawl "$url"))

for obj in "${cache[@]}"; do

    printf "\033[1;31m%s\033[0m\n" "$obj"
    [[ "$obj" =~ .*$domain[/]?$ ]] && continue
    [[ "$obj" =~ ^http.* ]] && crawl "$obj"
done
